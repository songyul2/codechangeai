
the cause of low precision: Imbalance classes
for example, ‘stable guest abi’ class has 3 messages.
10% of the data have multiple labels.

metrics used:
AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.
sklearn.metrics.precision_recall_fscore_support — scikit-learn 0.24.2 documentation

code details and hyperparameters:
cnn:
filter_sizes = [2, 3, 4, 5]
in fact, pick one from these numbers, such as        filter_sizes = [2]. 2 consecutive words will be considered. 
word_to_index, embedding_matrix have to match. embedding is essentially index to tensor.
nn.Dropout(0.5) is the largest reasonable value. larger dropout values will cause the train precision to be lower than valid.
GRU could be added to increase the model capacity.

dataset:
multilabel = 1 controls multilabel or single label prediction.
data = data.loc[data.label != "stable guest abi"] ignores this class with too few datapoints.
preprocess_stopwords does not affect the precision.

